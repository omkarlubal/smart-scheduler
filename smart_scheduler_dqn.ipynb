{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-07T18:30:06.250164Z",
     "start_time": "2024-03-07T18:29:58.373905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omkarlubal/alpha/PycharmProjects/shodai/.venv3.9/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T18:30:07.267009Z",
     "start_time": "2024-03-07T18:30:07.263691Z"
    }
   },
   "id": "ffc27a779aae6cd9",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, state_size, action_size, batch_size):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay_rate = 0.001\n",
    "        self.learning_rate = 0.01\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights()) \n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, actions_available, reward, next_state, done):\n",
    "        self.memory.append((state, action, actions_available, reward, next_state, done))\n",
    "    \n",
    "    def one_hot_state(self, state):\n",
    "        return np.reshape(state, [1, self.state_size])\n",
    "    \n",
    "    def masked_predict(self, model, state, actions_available):\n",
    "        act_values = model.predict(state, verbose=0)\n",
    "        # mask other actions as they are not available to assign to vm\n",
    "        for actions in actions_available:\n",
    "            act_values[0][actions] = 0\n",
    "        return act_values\n",
    "\n",
    "    def act(self, state, actions_available):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.env.sample()\n",
    "        act_values = self.masked_predict(self.model, state, actions_available)\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def replay(self, episode):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Initialize arrays for inputs and targets\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        targets = np.zeros((self.batch_size, self.action_size))\n",
    "        \n",
    "        for i, (state, action, actions_available, reward, next_state, done) in enumerate(minibatch):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.masked_predict(self.target_model, next_state, actions_available)[0]))\n",
    "            target_f = self.masked_predict(self.model, state, actions_available)\n",
    "            # update reward for a given action\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # Store the states and the targets\n",
    "            states[i] = state\n",
    "            targets[i] = target_f\n",
    "            \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_min + \\\n",
    "    (1 - self.epsilon_min) * np.exp(-self.epsilon_decay_rate*(8884+episode)*10) # added 8874 as the first training episode ended on 8874\n",
    "\n",
    "    def target_train(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        return self.epsilon\n",
    "        \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T18:36:34.012351Z",
     "start_time": "2024-03-07T18:36:33.999054Z"
    }
   },
   "id": "d9d4696ed11fe2d3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TaskWorkflow:\n",
    "    def __init__(self, adj_matrix, vms):\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.vms = vms\n",
    "        self.vm_status = np.zeros(len(vms))\n",
    "        self.max_vm = max(vms)\n",
    "\n",
    "    def reset(self):\n",
    "        for key in self.adj_matrix:\n",
    "            self.adj_matrix[key][\"finished\"] = False\n",
    "        \n",
    "        vm_status = [0] * len(self.vms)\n",
    "        return vm_status\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take an action in the environment and return the next state, reward, and whether the episode is done\n",
    "        # action -> assign a task in the vm\n",
    "        \n",
    "        next_state = self.vm_status\n",
    "        # base reward is max_vm used when terminal state\n",
    "        reward = self.max_vm\n",
    "        \n",
    "        # get any next available task\n",
    "        task_to_assign = self.get_next_available_tasks()\n",
    "        if len(task_to_assign) > 0:\n",
    "            \"\"\"\n",
    "            Strategy - one task at a step\n",
    "            1. Check if vm is free, then assign else free vm and then assign, first come random serve\n",
    "            \"\"\"\n",
    "            \n",
    "            self.adj_matrix[random.choice(list(task_to_assign))][\"finished\"] = True\n",
    "            # get any next available vm \n",
    "            free_vms = [idx for idx in range(0, len(self.vm_status)) if self.vm_status[idx]==0]\n",
    "            if len(free_vms) == 0:\n",
    "                # no vm is free, so select the next fastest available VM\n",
    "                vm_num = np.argmin(self.vm_status)\n",
    "                # also update remaining execution time for all other vms\n",
    "                # subtracting the selected min execution time\n",
    "                self.vm_status = np.maximum(self.vm_status - np.min(self.vm_status), 0)\n",
    "            else:\n",
    "                vm_num = random.choice(free_vms)\n",
    "            \n",
    "            selected_vm_capacity = self.vms[vm_num]\n",
    "            # update state: amount of time required to process it\n",
    "            self.vm_status[vm_num] = self.adj_matrix[action][\"load\"] / selected_vm_capacity\n",
    "            \n",
    "            # get reward from action\n",
    "            # high computing power -> low reward\n",
    "            reward = self.max_vm - selected_vm_capacity\n",
    "            next_state = self.vm_status\n",
    "            done = False\n",
    "            \n",
    "        else:\n",
    "            done = True\n",
    "        return next_state, reward, done\n",
    "        \n",
    "    def get_action_space_size(self):\n",
    "        return len(self.adj_matrix)\n",
    "    \n",
    "    def get_state_space_size(self):\n",
    "        return len(self.vms)\n",
    "    \n",
    "    def get_next_available_tasks(self):\n",
    "        # return a random task id\n",
    "        unfinished_tasks = set()\n",
    "    \n",
    "        # Start BFS from the root tasks (tasks with no dependencies)\n",
    "        queue = deque([task_number for task_number, task_data in self.adj_matrix.items() if task_data[\"depends\"] == -1])\n",
    "        # Perform BFS, assuming no cyclic dependency\n",
    "        while queue:\n",
    "            current_parent = queue.popleft()\n",
    "            if not self.adj_matrix[current_parent][\"finished\"]:\n",
    "                unfinished_tasks.add(current_parent)\n",
    "    \n",
    "            # Add next unfinished children tasks to the queue\n",
    "            for curr_task_number, curr_task_data in self.adj_matrix.items():\n",
    "                # check if child has parent dep, child task is unfinished, parent task is finished\n",
    "                if curr_task_data[\"depends\"] == int(current_parent): \n",
    "                    if curr_task_data[\"finished\"]:\n",
    "                        queue.append(curr_task_number)\n",
    "                    else:\n",
    "                        if self.adj_matrix[current_parent][\"finished\"]:\n",
    "                            unfinished_tasks.add(curr_task_number)\n",
    "        return unfinished_tasks\n",
    "    \n",
    "    def sample(self):\n",
    "        unfinished_tasks = self.get_next_available_tasks()\n",
    "        return -1 if len(unfinished_tasks) == 0 else random.choice(list(unfinished_tasks))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T18:36:36.951946Z",
     "start_time": "2024-03-07T18:36:36.940923Z"
    }
   },
   "id": "17ac9840fcd292cf",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "adj_matrix = {\n",
    "    0: {\"load\": 400, \"depends\": -1, \"finished\": True},\n",
    "    1: {\"load\": 300, \"depends\": 0, \"finished\": True},\n",
    "    2: {\"load\": 600, \"depends\": 0, \"finished\": False},\n",
    "    3: {\"load\": 200, \"depends\": 0, \"finished\": False},\n",
    "    4: {\"load\": 120, \"depends\": 1, \"finished\": False},\n",
    "    5: {\"load\": 100, \"depends\": 2, \"finished\": False},\n",
    "    6: {\"load\": 350, \"depends\": 3, \"finished\": False},\n",
    "    7: {\"load\": 800, \"depends\": 0, \"finished\": False},\n",
    "}\n",
    "\n",
    "vms = [50, 30, 10]\n",
    "\n",
    "env = TaskWorkflow(adj_matrix, vms)\n",
    "\n",
    "agent = DQNAgent(state_size=env.get_state_space_size(), action_size=env.get_action_space_size(), batch_size=64, env=env)\n",
    "\n",
    "# Training the DQN agent\n",
    "num_episodes = 10000\n",
    "total_start = time.time()\n",
    "weights_file_name = \"weights.h5\"\n",
    "if os.path.exists(weights_file_name):\n",
    "    agent.load(weights_file_name)\n",
    "    print(\"Loaded existing weights...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = agent.one_hot_state(env.reset())\n",
    "    reward_per_ep = 0\n",
    "    for time_step in range(100):  # Adjust the maximum time steps as needed\n",
    "        actions_available = env.get_next_available_tasks()\n",
    "        action = agent.act(state, actions_available)\n",
    "        next_state, reward, done, = env.step(action)\n",
    "        # print(next_state, reward, done, info, _)\n",
    "        reward_per_ep += reward\n",
    "        \n",
    "        next_state = agent.one_hot_state(next_state)\n",
    "        agent.remember(state, action, actions_available, reward, next_state, done)\n",
    "        agent.replay(episode)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            agent.target_train()\n",
    "            break\n",
    "    agent.save(weights_file_name)\n",
    "    end_time = time.time()\n",
    "    print(f\"{datetime.utcnow()} : Episode {str(episode)} total reward:{str(reward_per_ep)} avg reward: {str(reward_per_ep/500)} epsilon: {str(agent.get_epsilon())} time elapsed: {(end_time-total_start)/60:.2f}min\")\n",
    "total_end = time.time()\n",
    "print(f\"Total time: {total_end - total_start}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449ec4be417c2589",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9100ddb8eb4db8ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
